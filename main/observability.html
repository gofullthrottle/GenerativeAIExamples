<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Observability Tool &mdash; NVIDIA Generative AI Examples 24.6.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/version.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Press Release Chat Bot" href="notebooks/01_dataloader.html" />
    <link rel="prev" title="RAG Evaluation Application" href="tools/evaluation/index.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="index.html">
  <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-catalog.html">API Catalog Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector-database.html">Alternative Vector Database</a></li>
<li class="toctree-l1"><a class="reference internal" href="nim-llms.html">NIM for LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="simple-examples.html">Developing Simple Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="tools/evaluation/index.html">Evaluation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/01_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA API Catalog with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_Option%282%29_minimalistic_RAG_with_langchain_local_HF_LLM.html">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA API Catalog, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_Option%282%29_llama_index_with_HF_local_LLM.html">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/04_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Catelog and AI Catalog with LangChain Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/05_RAG_for_HTML_docs_with_Langchain_NVIDIA_AI_Endpoints.html">Build a RAG chain by generating embeddings for NVIDIA Triton documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/06_LangGraph_HandlingAgent_IntermediateSteps.html">LangGraph Handling LangChain Agent Intermediate_Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/07_Chat_with_nvidia_financial_reports.html">Notebook: Chatting with NVIDIA Financial Reports</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/08_RAG_Langchain_with_Local_NIM.html">Build a RAG using a locally hosted NIM</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/agentic_rag_with_nemo_retriever_nims.html">Agentic RAG pipeline with Nemo Retriever and LLM NIMs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="chain-server.html">Chain Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Observability Tool</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!--
  SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  SPDX-License-Identifier: Apache-2.0

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<section class="tex2jax_ignore mathjax_ignore" id="observability-tool">
<h1>Observability Tool<a class="headerlink" href="#observability-tool" title="Permalink to this headline">ÔÉÅ</a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction" id="id1">Introduction</a></p></li>
<li><p><a class="reference internal" href="#key-terms" id="id2">Key terms</a></p></li>
<li><p><a class="reference internal" href="#prerequisites" id="id3">Prerequisites</a></p></li>
<li><p><a class="reference internal" href="#build-and-start-the-containers" id="id4">Build and Start the Containers</a></p></li>
<li><p><a class="reference internal" href="#example-traces" id="id5">Example Traces</a></p>
<ul>
<li><p><a class="reference internal" href="#llamaindex-traces" id="id6">LlamaIndex Traces</a></p></li>
<li><p><a class="reference internal" href="#langchain-traces" id="id7">LangChain Traces</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#instrumenting-the-rag-playground" id="id8">Instrumenting the RAG Playground</a></p></li>
<li><p><a class="reference internal" href="#instrumenting-the-chain-server" id="id9">Instrumenting the Chain Server</a></p></li>
<li><p><a class="reference internal" href="#adding-observability-to-rag-applications" id="id10">Adding Observability to RAG Applications</a></p>
<ul>
<li><p><a class="reference internal" href="#setup" id="id11">Setup</a></p></li>
<li><p><a class="reference internal" href="#langchain" id="id12">LangChain</a></p></li>
<li><p><a class="reference internal" href="#llamaindex" id="id13">LLamaIndex</a></p></li>
</ul>
</li>
</ul>
</div>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>Observability is a crucial aspect that facilitates the monitoring and comprehension of the internal state and behavior of a system or application.
Applications based on RAG are intricate systems that encompass the interaction of several components.
To enhance the performance of these RAG-based applications, observability is an efficient mechanism for both monitoring and debugging.</p>
<p>The following diagram shows high-level overview of how OpenTelemetry captures the traces.</p>
<p><img alt="RAG with Observability" src="_images/image9.png" /></p>
<p>The following containers add the software components that enable observability. You can add containers and observability to any of the examples, such as using local GPUs, NVIDIA API Catalog Models, Query Decomposition, Multi-turn, and Multimodal.</p>
<ul class="simple">
<li><p>OpenTelemetry Collector: Receives, processes, and exports the traces.</p></li>
<li><p>Jaeger: Acts as an OpenTelemetry backend that provides storage, query service, and visualizer.
You can configure any other OTLP-compatible backend such as <a class="reference external" href="https://zipkin.io/">Zipkin</a>, <a class="reference external" href="https://prometheus.io/">Prometheus</a>, and so on.
To configure an alternative backend, refer to <a class="reference external" href="https://opentelemetry.io/docs/collector/configuration/">Configuration</a> in the OpenTelemetry documentation.</p></li>
<li><p>Cassandra: Provides persistent storage for traces.
Jaeger supports several more <a class="reference external" href="https://www.jaegertracing.io/docs/1.18/deployment/#storage-backends">storage backends</a> such as ElasticSearch, Kafka, and Badger.
For a large scale, production deployment, the Jaeger team recommends ElasticSearch over Cassandra.</p></li>
</ul>
</section>
<section id="key-terms">
<h2>Key terms<a class="headerlink" href="#key-terms" title="Permalink to this headline">ÔÉÅ</a></h2>
<dl class="simple myst">
<dt>Span</dt><dd><p>A unit of work within a system, encapsulating information about a specific operation (Eg. LLM call, embedding generation etc).</p>
</dd>
<dt>Traces</dt><dd><p>The recording of a request as it goes through a system, tracking every service the request comes in contact with.
Multiple spans make a trace logically bound by parent-child relationship.</p>
</dd>
<dt>Root Span</dt><dd><p>The first span in a trace, denoting the beginning and end of the entire operation.</p>
</dd>
<dt>Span Attributes</dt><dd><p>Key-value pairs a Span may consist of to provide additional context or metadata.</p>
</dd>
<dt>Collectors</dt><dd><p>Components that process and export telemetry data from instrumented applications.</p>
</dd>
<dt>Context</dt><dd><p>Signifies current location within the trace hierarchy.
The context determines whether a new span initiates a trace or connects to an existing parent span.</p>
</dd>
<dt>Services</dt><dd><p>Microservices that generates telemetry data.</p>
</dd>
</dl>
<p>The following diagram shows a typical trace for query that uses a knowledge base and identifies the spans and root span.</p>
<p><img alt="Trace for query from knowledge base" src="_images/image10.png" /></p>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul>
<li><p>Clone the Generative AI examples Git repository using Git LFS:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>apt<span class="w"> </span>-y<span class="w"> </span>install<span class="w"> </span>git-lfs
<span class="gp">$ </span>git<span class="w"> </span>clone<span class="w"> </span>git@github.com:NVIDIA/GenerativeAIExamples.git
<span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>GenerativeAIExamples/
<span class="gp">$ </span>git<span class="w"> </span>lfs<span class="w"> </span>pull
</pre></div>
</div>
</li>
<li><p>A host with an NVIDIA A100, H100, or L40S GPU.</p></li>
<li><p>Verify NVIDIA GPU driver version 535 or later is installed and that the GPU is in compute mode:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>nvidia-smi<span class="w"> </span>-q<span class="w"> </span>-d<span class="w"> </span>compute
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">==============NVSMI LOG==============</span>

<span class="go">Timestamp                                 : Sun Nov 26 21:17:25 2023</span>
<span class="hll"><span class="go">Driver Version                            : 535.129.03</span>
</span><span class="go">CUDA Version                              : 12.2</span>

<span class="go">Attached GPUs                             : 1</span>
<span class="go">GPU 00000000:CA:00.0</span>
<span class="hll"><span class="go">    Compute Mode                          : Default</span>
</span></pre></div>
</div>
<p>If the driver is not installed or below version 535, refer to the <a class="reference external" href="https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html"><em>NVIDIA Driver Installation Quickstart Guide</em></a>.</p>
</li>
<li><p>Install Docker Engine and Docker Compose.
Refer to the instructions for <a class="reference external" href="https://docs.docker.com/engine/install/ubuntu/">Ubuntu</a>.</p></li>
<li><p>Install the NVIDIA Container Toolkit.</p>
<ol class="arabic">
<li><p>Refer to the <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">installation documentation</a>.</p></li>
<li><p>When you configure the runtime, set the NVIDIA runtime as the default:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>nvidia-ctk<span class="w"> </span>runtime<span class="w"> </span>configure<span class="w"> </span>--runtime<span class="o">=</span>docker<span class="w"> </span>--set-as-default
</pre></div>
</div>
<p>If you did not set the runtime as the default, you can reconfigure the runtime by running the preceding command.</p>
</li>
<li><p>Verify the NVIDIA container toolkit is installed and configured as the default container runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>/etc/docker/daemon.json
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;default-runtime&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;nvidia&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;runtimes&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;nvidia&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;args&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">            </span><span class="nt">&quot;path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;nvidia-container-runtime&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Run the <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> command in a container to verify the configuration:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>--runtime<span class="o">=</span>nvidia<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>ubuntu<span class="w"> </span>nvidia-smi<span class="w"> </span>-L
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">GPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-d8ce95c1-12f7-3174-6395-e573163a2ace)</span>
</pre></div>
</div>
</li>
</ol>
</li>
</ul>
</section>
<section id="build-and-start-the-containers">
<h2>Build and Start the Containers<a class="headerlink" href="#build-and-start-the-containers" title="Permalink to this headline">ÔÉÅ</a></h2>
<ol class="arabic">
<li><p>In the Generative AI Examples repository, edit the <code class="docutils literal notranslate"><span class="pre">deploy/compose/configs/otel-collector-config.yaml</span></code> and <code class="docutils literal notranslate"><span class="pre">deploy/compose/configs/jaeger.yaml</span></code> files.</p>
<p>Refer to <a class="reference external" href="https://opentelemetry.io/docs/collector/configuration/">configuration</a> in the OpenTelemetry documentation and the <a class="reference external" href="https://www.jaegertracing.io/docs/1.52/cli/#jaeger-all-in-one-cassandra">Jaeger all-in-one with Cassandra</a>
reference in the Jaeger documentation.</p>
</li>
<li><p>Set following environment variables in the chain-server and rag-playground services of the corresponding example in Docker Compose file.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">environment</span><span class="p">:</span>
<span class="w">  </span><span class="nt">OTEL_EXPORTER_OTLP_ENDPOINT</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http://otel-collector:4317</span>
<span class="w">  </span><span class="nt">OTEL_EXPORTER_OTLP_PROTOCOL</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">grpc</span>
<span class="w">  </span><span class="nt">ENABLE_TRACING</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
</li>
<li><p>Deploy the RAG example:</p>
<ul>
<li><p>Developer RAG Text chat bot:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span>build
<span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span>up<span class="w"> </span>-d
</pre></div>
</div>
</li>
<li><p>NVIDIA API Catalog Text chat bot:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-api-catalog-text-chatbot.yaml<span class="w"> </span>build
<span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-api-catalog-text-chatbot.yaml<span class="w"> </span>up<span class="w"> </span>-d
</pre></div>
</div>
</li>
<li><p>Multimodal chat bot:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-multimodal-chatbot.yaml<span class="w"> </span>build
<span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-multimodal-chatbot.yaml<span class="w"> </span>up<span class="w"> </span>-d
</pre></div>
</div>
</li>
<li><p>Multi-turn chat bot:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-multiturn-chatbot.yaml<span class="w"> </span>build
<span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-multiturn-chatbot.yaml<span class="w"> </span>up<span class="w"> </span>-d
</pre></div>
</div>
</li>
<li><p>Query Decomposition Agent:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-query-decomposition-agent.yaml<span class="w"> </span>build
<span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-query-decomposition-agent.yaml<span class="w"> </span>up<span class="w"> </span>-d
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Start the vector database used in the preceding example:</p>
<ul>
<li><p>Milvus:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/docker-compose-vectordb.yaml<span class="w"> </span>up<span class="w"> </span>-d<span class="w"> </span>milvus
</pre></div>
</div>
</li>
<li><p>Pgvector:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/docker-compose-vectordb.yaml<span class="w"> </span>up<span class="w"> </span>-d<span class="w"> </span>pgvector
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Deploy the observability services:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/docker-compose-observability.yaml<span class="w"> </span>build
<span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/docker-compose-observability.yaml<span class="w"> </span>up<span class="w"> </span>-d
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">‚úî Container otel-collector              Started</span>
<span class="go">‚úî Container cassandra                   Started</span>
<span class="go">‚úî Container compose-cassandra-schema-1  Started</span>
<span class="go">‚úî Container jaeger                      Started</span>
</pre></div>
</div>
</li>
<li><p>Optional: Confirm the services are running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>ps<span class="w"> </span>--format<span class="w"> </span><span class="s2">&quot;table {{.ID}}\t{{.Names}}\t{{.Status}}&quot;</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">CONTAINER ID   NAMES               STATUS</span>
<span class="go">beb1582320d6   jaeger              Up 5 minutes</span>
<span class="go">674c7bbb367e   cassandra           Up 6 minutes</span>
<span class="go">d11e35ee69f4   rag-playground      Up 5 minutes</span>
<span class="go">68f22b3842cb   chain-server        Up 5 minutes</span>
<span class="go">751dd4fd80ec   milvus-standalone   Up 5 minutes (healthy)</span>
<span class="go">b435006c95c1   milvus-minio        Up 6 minutes (healthy)</span>
<span class="go">9108253d058d   notebook-server     Up 6 minutes</span>
<span class="go">5315a9dc9eb4   milvus-etcd         Up 6 minutes (healthy)</span>
<span class="go">d314a43074c8   otel-collector      Up 6 minutes</span>
</pre></div>
</div>
</li>
<li><p>Access the Jaeger web interface at <code class="docutils literal notranslate"><span class="pre">http://host-ip:16686</span></code> from your web browser.</p></li>
</ol>
</section>
<section id="example-traces">
<h2>Example Traces<a class="headerlink" href="#example-traces" title="Permalink to this headline">ÔÉÅ</a></h2>
<section id="llamaindex-traces">
<h3>LlamaIndex Traces<a class="headerlink" href="#llamaindex-traces" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>The following screenshots show traces of LlamaIndex based examples from the Jaeger web interface.</p>
<ul class="simple">
<li><p>Upload document trace
<img alt="upload document trace" src="_images/image11.png" /></p></li>
<li><p>User query using knowledge base trace
<img alt="user query using knowledge base" src="_images/image12.png" /></p></li>
</ul>
</section>
<section id="langchain-traces">
<h3>LangChain Traces<a class="headerlink" href="#langchain-traces" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>NVIDIA API Catalog Example: User query without using knowledge base
<img alt="API catalog example without using knowledge base" src="_images/image13.png" /></p></li>
<li><p>Multimodal example: Upload document with graphs and images
<img alt="Multimodal example upload document" src="_images/image14.png" /></p></li>
<li><p>Query decomposition example: User query using knowledge base
<img alt="Query decomposition example user query" src="_images/image15.png" /></p></li>
</ul>
</section>
</section>
<section id="instrumenting-the-rag-playground">
<h2>Instrumenting the RAG Playground<a class="headerlink" href="#instrumenting-the-rag-playground" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>The <a class="reference external" href="https://github.com/NVIDIA/GenerativeAIExamples/blob/main/RetrievalAugmentedGeneration/frontend/frontend/tracing.py">tracing.py</a> module in the frontend application code performs the instrumentation.
At high level, the code performs the following:</p>
<ul class="simple">
<li><p>Sets up the OpenTelemetry configurations for resource name, frontend, span processor, and context propagator.</p></li>
<li><p>Provides instrumentation decorator functions, <code class="docutils literal notranslate"><span class="pre">instrumentation_wrapper</span></code> and <code class="docutils literal notranslate"><span class="pre">predict_instrumentation_wrapper</span></code>, for abstracting away the opentelemetry configurations and managing trace context across different services.
The API functions in <a class="reference external" href="https://github.com/NVIDIA/GenerativeAIExamples/blob/main/RetrievalAugmentedGeneration/frontend/frontend/chat_client.py">chat_client.py</a> use the decorator functions to create new span contexts.
The decorator functions inject the span contexts into the headers of the requests made to the chain server and export the span information and extracted attributes to the OpenTelemetry collector.</p></li>
</ul>
</section>
<section id="instrumenting-the-chain-server">
<h2>Instrumenting the Chain Server<a class="headerlink" href="#instrumenting-the-chain-server" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>The <a class="reference external" href="https://github.com/NVIDIA/GenerativeAIExamples/blob/main/RetrievalAugmentedGeneration/common/tracing.py">tracing.py</a> module in the chain server application code performs the instrumentation.
At high level, the code performs the following:</p>
<ul class="simple">
<li><p>Sets up the OpenTelemetry configurations for resource name, chain-server, span processor, and context propagator.</p></li>
<li><p>Initializes the LlamaIndex OpenTelemetry callback handler in <a class="reference external" href="https://github.com/NVIDIA/GenerativeAIExamples/blob/main/tools/observability/llamaindex/opentelemetry_callback.py">opentelemetry_callback.py</a>.
The callback handler uses <a class="reference external" href="https://docs.llamaindex.ai/en/stable/module_guides/observability/callbacks/root.html">LlamaIndex callbacks</a> to track various events such as LLM calls, chunking, embedding, and so on.</p></li>
<li><p>Provides an instrumentation decorator functions, <code class="docutils literal notranslate"><span class="pre">llamaindex_instrumentation_wrapper</span></code>, <code class="docutils literal notranslate"><span class="pre">langchain_instrumentation_method_wrapper</span></code> and <code class="docutils literal notranslate"><span class="pre">langchain_instrumentation_class_wrapper</span></code> for abstracting away the opentelemetry configurations and managing the context.
The API functions in <a class="reference external" href="https://github.com/NVIDIA/GenerativeAIExamples/blob/main/RetrievalAugmentedGeneration/common/server.py">server.py</a> use the decorator function to extract the trace context that is present in requests from the frontend service and attach it in the new span created by the chain-server.</p></li>
</ul>
<p>You can use the decorator function, <code class="docutils literal notranslate"><span class="pre">llamaindex_instrumentation_wrapper</span></code>, to instrument any LlamaIndex application as long as you set the LlamaIndex OpenTelemetry callback handler, <code class="docutils literal notranslate"><span class="pre">opentelemetry_callback.py</span></code>, as global handler in the application.</p>
<p>Similarly, you can use the decorator functions <code class="docutils literal notranslate"><span class="pre">langchain_instrumentation_method_wrapper</span></code> and <code class="docutils literal notranslate"><span class="pre">langchain_instrumentation_class_wrapper</span></code> for instrumenting any Langchain applications as long as you set the Langchain OpenTelemetry callback handler, <code class="docutils literal notranslate"><span class="pre">opentelemetry_callback.py</span></code>, in the <a class="reference external" href="https://github.com/NVIDIA/GenerativeAIExamples/blob/main/RetrievalAugmentedGeneration/common/tracing.py">tracing.py</a> module.</p>
</section>
<section id="adding-observability-to-rag-applications">
<h2>Adding Observability to RAG Applications<a class="headerlink" href="#adding-observability-to-rag-applications" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>To extend the observability support to other LLM and RAG applications using the
<a class="reference external" href="https://github.com/NVIDIA/GenerativeAIExamples/blob/main/tools/observability/langchain/">LangChain callback handlers</a>
or <a class="reference external" href="https://github.com/NVIDIA/GenerativeAIExamples/blob/maintools/observability/llamaindex/">LlamaIndex callback handlers</a>,
refer to the following sections.</p>
<section id="setup">
<h3>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the dependencies</span>
<span class="kn">from</span> <span class="nn">opentelemetry</span> <span class="kn">import</span> <span class="n">trace</span>
<span class="kn">from</span> <span class="nn">opentelemetry.sdk.resources</span> <span class="kn">import</span> <span class="n">SERVICE_NAME</span><span class="p">,</span> <span class="n">Resource</span>
<span class="kn">from</span> <span class="nn">opentelemetry.sdk.trace</span> <span class="kn">import</span> <span class="n">TracerProvider</span>
<span class="kn">from</span> <span class="nn">opentelemetry.sdk.trace.export</span> <span class="kn">import</span> <span class="n">SimpleSpanProcessor</span>
<span class="kn">from</span> <span class="nn">opentelemetry.exporter.otlp.proto.grpc.trace_exporter</span> <span class="kn">import</span> <span class="n">OTLPSpanExporter</span>
<span class="kn">from</span> <span class="nn">opentelemetry.trace.propagation.tracecontext</span> <span class="kn">import</span> <span class="n">TraceContextTextMapPropagator</span>

<span class="c1"># Setup the tracer</span>
<span class="n">OTEL_EXPORTER_OTLP_ENDPOINT</span> <span class="o">=</span> <span class="s2">&quot;localhost:4317&quot;</span>
<span class="n">resource</span> <span class="o">=</span> <span class="n">Resource</span><span class="o">.</span><span class="n">create</span><span class="p">({</span><span class="n">SERVICE_NAME</span><span class="p">:</span> <span class="s2">&quot;sample_service&quot;</span><span class="p">})</span>
<span class="n">provider</span> <span class="o">=</span> <span class="n">TracerProvider</span><span class="p">(</span><span class="n">resource</span><span class="o">=</span><span class="n">resource</span><span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">SimpleSpanProcessor</span><span class="p">(</span><span class="n">OTLPSpanExporter</span><span class="p">(</span><span class="n">endpoint</span><span class="o">=</span><span class="n">OTEL_EXPORTER_OTLP_ENDPOINT</span><span class="p">,</span> <span class="n">insecure</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">provider</span><span class="o">.</span><span class="n">add_span_processor</span><span class="p">(</span><span class="n">processor</span><span class="p">)</span>
<span class="n">trace</span><span class="o">.</span><span class="n">set_tracer_provider</span><span class="p">(</span><span class="n">provider</span><span class="p">)</span>
<span class="n">tracer</span> <span class="o">=</span> <span class="n">trace</span><span class="o">.</span><span class="n">get_tracer</span><span class="p">(</span><span class="s2">&quot;sample_tracer&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="langchain">
<h3>LangChain<a class="headerlink" href="#langchain" title="Permalink to this headline">ÔÉÅ</a></h3>
<ol class="arabic">
<li><p>Copy the LangChain OpenTelemetry callback handler, <a class="reference external" href="https://github.com/NVIDIA/GenerativeAIExamples/blob/main/tools/observability/langchain/opentelemetry_callback.py">opentelemetry_callback.py</a>, module in your project.
For example copy it in <code class="docutils literal notranslate"><span class="pre">utils/observability</span></code> directory.</p></li>
<li><p>Instantiate the LangChain OpenTelemetry callback handler:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils.observability.opentelemetry_callback</span> <span class="kn">import</span> <span class="n">OpenTelemetryCallbackHandler</span>
<span class="n">otel_handler</span> <span class="o">=</span> <span class="n">OpenTelemetryCallbackHandler</span><span class="p">(</span><span class="n">tracer</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Use the OTEL handler.</p>
<ul>
<li><p>Simple Sequential Chain</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">LLMChain</span><span class="p">,</span> <span class="n">SimpleSequentialChain</span>
<span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
<span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are a playwright. Given the title of play, it is your job to write a synopsis for that title.</span>
<span class="s2">    Title: </span><span class="si">{title}</span>
<span class="s2">    Playwright: This is a synopsis for the above play:&quot;&quot;&quot;</span>
<span class="n">prompt_template</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span><span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;title&quot;</span><span class="p">],</span> <span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">)</span>
<span class="n">synopsis_chain</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">)</span>
<span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.</span>
<span class="s2">    Play Synopsis:</span>
<span class="s2">    </span><span class="si">{synopsis}</span>
<span class="s2">    Review from a New York Times play critic of the above play:&quot;&quot;&quot;</span>
<span class="n">prompt_template</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span><span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;synopsis&quot;</span><span class="p">],</span> <span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">)</span>
<span class="n">review_chain</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">)</span>
<span class="n">overall_chain</span> <span class="o">=</span> <span class="n">SimpleSequentialChain</span><span class="p">(</span>
    <span class="n">chains</span><span class="o">=</span><span class="p">[</span><span class="n">synopsis_chain</span><span class="p">,</span> <span class="n">review_chain</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># invoke</span>
<span class="n">review</span> <span class="o">=</span> <span class="n">overall_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Tragedy at sunset on the beach&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;callbacks&quot;</span><span class="p">:[</span><span class="n">otel_handler</span><span class="p">]})</span> <span class="c1"># add the otel handler to the run method</span>
<span class="c1"># run</span>
<span class="n">review</span> <span class="o">=</span> <span class="n">overall_chain</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;Tragedy at sunset on the beach&quot;</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">otel_handler</span><span class="p">])</span> <span class="c1"># add the otel handler to the run method</span>
</pre></div>
</div>
</li>
<li><p>Sequential Chain in LangChain Expression Language (LCEL)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain.schema</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>

<span class="n">prompt1</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;what is the city </span><span class="si">{person}</span><span class="s2"> is from?&quot;</span><span class="p">)</span>
<span class="n">prompt2</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span>
    <span class="s2">&quot;what country is the city </span><span class="si">{city}</span><span class="s2"> in? respond in </span><span class="si">{language}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">()</span>
<span class="n">chain1</span> <span class="o">=</span> <span class="n">prompt1</span> <span class="o">|</span> <span class="n">model</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="n">chain2</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;city&quot;</span><span class="p">:</span> <span class="n">chain1</span><span class="p">,</span> <span class="s2">&quot;language&quot;</span><span class="p">:</span> <span class="n">itemgetter</span><span class="p">(</span><span class="s2">&quot;language&quot;</span><span class="p">)}</span>
    <span class="o">|</span> <span class="n">prompt2</span>
    <span class="o">|</span> <span class="n">model</span>
    <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">chain2</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;person&quot;</span><span class="p">:</span> <span class="s2">&quot;obama&quot;</span><span class="p">,</span> <span class="s2">&quot;language&quot;</span><span class="p">:</span> <span class="s2">&quot;spanish&quot;</span><span class="p">},</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;callbacks&quot;</span><span class="p">:[</span><span class="n">otel_handler</span><span class="p">]})</span>
</pre></div>
</div>
</li>
<li><p>Agent</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.agents</span> <span class="kn">import</span> <span class="n">AgentExecutor</span><span class="p">,</span> <span class="n">load_tools</span><span class="p">,</span> <span class="n">create_openai_functions_agent</span>
<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">langchain</span> <span class="kn">import</span> <span class="n">hub</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tools</span> <span class="o">=</span> <span class="n">load_tools</span><span class="p">([</span><span class="s2">&quot;serpapi&quot;</span><span class="p">])</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">pull</span><span class="p">(</span><span class="s2">&quot;hwchase17/openai-functions-agent&quot;</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">create_openai_functions_agent</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">tools</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span>
<span class="n">agent_executor</span> <span class="o">=</span> <span class="n">AgentExecutor</span><span class="p">(</span><span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span> <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">)</span>

<span class="n">agent_executor</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;What is Langfuse?&quot;</span><span class="p">},</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;callbacks&quot;</span><span class="p">:[</span><span class="n">otel_handler</span><span class="p">]})</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ol>
</section>
<section id="llamaindex">
<h3>LLamaIndex<a class="headerlink" href="#llamaindex" title="Permalink to this headline">ÔÉÅ</a></h3>
<ol class="arabic">
<li><p>Copy the LangChain OpenTelemetry Callback handler, <a class="reference external" href="https://github.com/NVIDIA/GenerativeAIExamples/blob/main/tools/observability/llamaindex/opentelemetry_callback.py">opentelemetry_callback.py</a>, module in your project. For example copy it in <code class="docutils literal notranslate"><span class="pre">utils/observability</span></code>
directory.</p></li>
<li><p>Import <code class="docutils literal notranslate"><span class="pre">OpenTelemetryCallbackHandler</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils.observability.opentelemetry_callback</span> <span class="kn">import</span> <span class="n">OpenTelemetryCallbackHandler</span>
<span class="n">otel_handler</span> <span class="o">=</span> <span class="n">OpenTelemetryCallbackHandler</span><span class="p">(</span><span class="n">tracer</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Use the OTEL handler.</p>
<ol class="arabic">
<li><p>Download data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!mkdir -p &#39;data/paul_graham/&#39;
!wget &#39;https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt&#39; -O &#39;data/paul_graham/paul_graham_essay.txt&#39;
</pre></div>
</div>
</li>
<li><p>Set the callback handler:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index.core.callbacks</span> <span class="kn">import</span> <span class="n">CallbackManager</span>
<span class="kn">from</span> <span class="nn">llama_index.core</span> <span class="kn">import</span> <span class="n">SummaryIndex</span>
<span class="kn">from</span> <span class="nn">llama_index.core</span> <span class="kn">import</span> <span class="n">SimpleDirectoryReader</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">SimpleDirectoryReader</span><span class="p">(</span><span class="s2">&quot;./data/paul_graham&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="c1"># Option 1: Explicitly use callback handler in the query engine</span>
<span class="n">callback_manager</span> <span class="o">=</span> <span class="n">CallbackManager</span><span class="p">([</span><span class="n">otel_handler</span><span class="p">])</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">SummaryIndex</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">callback_manager</span><span class="o">=</span><span class="n">callback_manager</span><span class="p">)</span>
<span class="n">query_engine</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">as_query_engine</span><span class="p">()</span>

<span class="c1"># OR</span>

<span class="c1"># Option 2: Set OpenTelemetry handler as global callback handler</span>
<span class="kn">import</span> <span class="nn">llama_index</span>
<span class="n">llama_index</span><span class="o">.</span><span class="n">global_handler</span> <span class="o">=</span> <span class="n">OpenTelemetryCallbackHandler</span><span class="p">(</span><span class="n">tracer</span><span class="p">)</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">SummaryIndex</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="n">query_engine</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">as_query_engine</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ol>
</li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="tools/evaluation/index.html" class="btn btn-neutral float-left" title="RAG Evaluation Application" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="notebooks/01_dataloader.html" class="btn btn-neutral float-right" title="Press Release Chat Bot" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA Corporation.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>