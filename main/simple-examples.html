<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Developing Simple Examples &mdash; NVIDIA Generative AI Examples 24.6.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/version.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Setting up the Simple Example" href="boilerplate-api-catalog.html" />
    <link rel="prev" title="Using NVIDIA NIM for LLMs" href="nim-llms.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="index.html">
  <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-catalog.html">API Catalog Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector-database.html">Alternative Vector Database</a></li>
<li class="toctree-l1"><a class="reference internal" href="nim-llms.html">NIM for LLMs</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Developing Simple Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="boilerplate-api-catalog.html">Setting up the Simple Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="ingest-api-catalog.html">Implementing the Ingest Docs Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="documents-api-catalog.html">Listing and Searching Documents</a></li>
<li class="toctree-l2"><a class="reference internal" href="llm-api-catalog.html">Creating an LLM Chain</a></li>
<li class="toctree-l2"><a class="reference internal" href="rag-api-catalog.html">Creating a RAG Chain</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tools/evaluation/index.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="observability.html">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/01_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA API Catalog with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_Option%282%29_minimalistic_RAG_with_langchain_local_HF_LLM.html">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA API Catalog, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_Option%282%29_llama_index_with_HF_local_LLM.html">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/04_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Catelog and AI Catalog with LangChain Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/05_RAG_for_HTML_docs_with_Langchain_NVIDIA_AI_Endpoints.html">Build a RAG chain by generating embeddings for NVIDIA Triton documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/06_LangGraph_HandlingAgent_IntermediateSteps.html">LangGraph Handling LangChain Agent Intermediate_Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/07_Chat_with_nvidia_financial_reports.html">Notebook: Chatting with NVIDIA Financial Reports</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/08_RAG_Langchain_with_Local_NIM.html">Build a RAG using a locally hosted NIM</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/agentic_rag_with_nemo_retriever_nims.html">Agentic RAG pipeline with Nemo Retriever and LLM NIMs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="chain-server.html">Chain Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Developing Simple Examples</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!--
  SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  SPDX-License-Identifier: Apache-2.0

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<section class="tex2jax_ignore mathjax_ignore" id="developing-simple-examples">
<h1>Developing Simple Examples<a class="headerlink" href="#developing-simple-examples" title="Permalink to this headline"></a></h1>
<section id="about-the-purpose-and-process">
<h2>About the Purpose and Process<a class="headerlink" href="#about-the-purpose-and-process" title="Permalink to this headline"></a></h2>
<p>The purpose of the example is to show how to develop a RAG example.
The example uses models from the NVIDIA API Catalog.
Using models from the catalog simplifies the initial setup by avoiding the steps to download a model and run an inference server.
The endpoints from the catalog serve as both an embedding and an inference server.</p>
<p>The following pages show the sample code for gradually implementing the methods of a chain server that use LlamaIndex utility functions.</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="boilerplate-api-catalog.html">Setting up the Simple Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="ingest-api-catalog.html">Implementing the Ingest Docs Method</a></li>
<li class="toctree-l1"><a class="reference internal" href="documents-api-catalog.html">Listing and Searching Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm-api-catalog.html">Creating an LLM Chain</a></li>
<li class="toctree-l1"><a class="reference internal" href="rag-api-catalog.html">Creating a RAG Chain</a></li>
</ul>
</div>
</section>
<section id="key-considerations-for-the-simple-rag">
<h2>Key Considerations for the Simple RAG<a class="headerlink" href="#key-considerations-for-the-simple-rag" title="Permalink to this headline"></a></h2>
<p>The simple example uses the build and configuration approach that NVIDIA provided examples use.
Reusing the build and configuration enables the sample to focus on the basics of developing the RAG code.</p>
<p>The key consideration for the RAG code is to implement three required methods and one optional method:</p>
<dl class="simple myst">
<dt>ingest_docs</dt><dd><p>The chain server run this method when you upload documents to use as a knowledge base.</p>
</dd>
<dt>llm_chain</dt><dd><p>The chain server runs this method when a query does not rely on retrieval.</p>
</dd>
<dt>rag_chain</dt><dd><p>The chain server runs this method when you request to use the knowledge base and use retrieval to answer a query.</p>
</dd>
<dt>get_documents</dt><dd><p>The chain server runs this method when you access the <code class="docutils literal notranslate"><span class="pre">/documents</span></code> endpoint with a GET request.</p>
</dd>
<dt>delete_documents</dt><dd><p>The chain server runs this method when you access the <code class="docutils literal notranslate"><span class="pre">/documents</span></code> endpoint with a DELETE request.</p>
</dd>
<dt>document_search</dt><dd><p>This is an optional method that enables you to perform the same document search that the <code class="docutils literal notranslate"><span class="pre">rag_chain</span></code> method runs.</p>
</dd>
</dl>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline"></a></h2>
<p>The following prerequisites are common for using models from the NVIDIA API Catalog.
If you already performed these steps, you do not need to repeat them.</p>
<ul>
<li><p>Clone the Generative AI examples Git repository using Git LFS:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>apt<span class="w"> </span>-y<span class="w"> </span>install<span class="w"> </span>git-lfs
<span class="gp">$ </span>git<span class="w"> </span>clone<span class="w"> </span>git@github.com:NVIDIA/GenerativeAIExamples.git
<span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>GenerativeAIExamples/
<span class="gp">$ </span>git<span class="w"> </span>lfs<span class="w"> </span>pull
</pre></div>
</div>
</li>
<li><p>Install Docker Engine and Docker Compose.
Refer to the instructions for <a class="reference external" href="https://docs.docker.com/engine/install/ubuntu/">Ubuntu</a>.</p></li>
<li><p>Login to Nvidia’s docker registry. Please refer to <a class="reference external" href="https://docs.nvidia.com/ngc/gpu-cloud/ngc-overview/index.html">instructions</a> to create account and generate NGC API key. This is needed for pulling in the secure base container used by all the examples.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>login<span class="w"> </span>nvcr.io
<span class="go">Username: $oauthtoken</span>
<span class="go">Password: &lt;ngc-api-key&gt;</span>
</pre></div>
</div>
</li>
<li><p>Optional: Enable NVIDIA Riva automatic speech recognition (ASR) and text to speech (TTS).</p>
<ul>
<li><p>To launch a Riva server locally, refer to the <a class="reference external" href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html">Riva Quick Start Guide</a>.</p>
<ul class="simple">
<li><p>In the provided <code class="docutils literal notranslate"><span class="pre">config.sh</span></code> script, set <code class="docutils literal notranslate"><span class="pre">service_enabled_asr=true</span></code> and <code class="docutils literal notranslate"><span class="pre">service_enabled_tts=true</span></code>, and select the desired ASR and TTS languages by adding the appropriate language codes to <code class="docutils literal notranslate"><span class="pre">asr_language_code</span></code> and <code class="docutils literal notranslate"><span class="pre">tts_language_code</span></code>.</p></li>
<li><p>After the server is running, assign its IP address (or hostname) and port (50051 by default) to <code class="docutils literal notranslate"><span class="pre">RIVA_API_URI</span></code> in <code class="docutils literal notranslate"><span class="pre">deploy/compose/compose.env</span></code>.</p></li>
</ul>
</li>
<li><p>Alternatively, you can use a hosted Riva API endpoint. You might need to obtain an API key and/or Function ID for access.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">deploy/compose/compose.env</span></code>, make the following assignments as necessary:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">RIVA_API_URI</span><span class="o">=</span><span class="s2">&quot;&lt;riva-api-address/hostname&gt;:&lt;port&gt;&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RIVA_API_KEY</span><span class="o">=</span><span class="s2">&quot;&lt;riva-api-key&gt;&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RIVA_FUNCTION_ID</span><span class="o">=</span><span class="s2">&quot;&lt;riva-function-id&gt;&quot;</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
<section id="get-an-nvidia-api-key">
<h2>Get an NVIDIA API Key<a class="headerlink" href="#get-an-nvidia-api-key" title="Permalink to this headline"></a></h2>
<p>The following steps describe how to get an NVIDIA API key for the Mixtral 8x7B Instruct API Endpoint.
If you already performed these steps, you do not need to repeat them.</p>
<p>Perform the following steps if you do not already have an API key.
You can use different model API endpoints with the same API key.</p>
<ol class="arabic">
<li><p>Navigate to <a class="reference external" href="https://build.nvidia.com/explore/discover">https://build.nvidia.com/explore/discover</a>.</p></li>
<li><p>Find the <strong>Llama 3 70B Instruct</strong> card and click the card.</p>
<p><img alt="Llama 3 70B Instruct model card" src="_images/llama3-70b-instruct-model-card.png" /></p>
</li>
<li><p>Click <strong>Get API Key</strong>.</p>
<p><img alt="API section of the model page." src="_images/llama3-70b-instruct-get-api-key.png" /></p>
</li>
<li><p>Click <strong>Generate Key</strong>.</p>
<p><img alt="Generate key window." src="_images/api-catalog-generate-api-key.png" /></p>
</li>
<li><p>Click <strong>Copy Key</strong> and then save the API key.
The key begins with the letters nvapi-.</p>
<p><img alt="Key Generated window." src="_images/key-generated.png" /></p>
</li>
</ol>
</section>
<section id="custom-requirements">
<h2>Custom Requirements<a class="headerlink" href="#custom-requirements" title="Permalink to this headline"></a></h2>
<p>The sample code for implementing the Chain Server is basic.
As you experiment and implement custom requirements, you might need to use additional Python packages.
You can install your Python dependencies by creating a <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> file in your example directory.
When you build the Docker image, the build process automatically installs the packages from the requirements file.</p>
<p>You might find conflicts in the dependencies from the requirements file and the common dependencies used by the Chain Server.
In these cases, the custom requirements file is given a higher priority when Python resolves dependencies.
Your packages and versions can break some of the functionality of the utility methods in the chain server.</p>
<p>If your example uses any utility methods, check the chain server logs to troubleshoot dependency conflicts.
If any of the packages required by the utility methods causes an error, the error is logged by the chain server during initialization.
These errors do not stop the execution of the chain server.
However, if your example attempts to use a utility method that depends on a broken package, the chain server can produce unexpected behavior or crashes.
You must use dependencies that do not conflict with the chain server requirements, or do not rely on utility methods that are affected by the dependency conflict.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="nim-llms.html" class="btn btn-neutral float-left" title="Using NVIDIA NIM for LLMs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="boilerplate-api-catalog.html" class="btn btn-neutral float-right" title="Setting up the Simple Example" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA Corporation.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>