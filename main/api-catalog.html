<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using the NVIDIA API Catalog &mdash; NVIDIA Generative AI Examples 24.6.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/version.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Query Decomposition" href="query-decomposition.html" />
    <link rel="prev" title="Support Matrix" href="support-matrix.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="index.html">
  <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">API Catalog Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector-database.html">Alternative Vector Database</a></li>
<li class="toctree-l1"><a class="reference internal" href="nim-llms.html">NIM for LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="simple-examples.html">Developing Simple Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tools/evaluation/index.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="observability.html">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/01_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA API Catalog with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_Option%282%29_minimalistic_RAG_with_langchain_local_HF_LLM.html">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA API Catalog, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_Option%282%29_llama_index_with_HF_local_LLM.html">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/04_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Catelog and AI Catalog with LangChain Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/05_RAG_for_HTML_docs_with_Langchain_NVIDIA_AI_Endpoints.html">Build a RAG chain by generating embeddings for NVIDIA Triton documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/06_LangGraph_HandlingAgent_IntermediateSteps.html">LangGraph Handling LangChain Agent Intermediate_Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/07_Chat_with_nvidia_financial_reports.html">Notebook: Chatting with NVIDIA Financial Reports</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/08_RAG_Langchain_with_Local_NIM.html">Build a RAG using a locally hosted NIM</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/agentic_rag_with_nemo_retriever_nims.html">Agentic RAG pipeline with Nemo Retriever and LLM NIMs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="chain-server.html">Chain Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Using the NVIDIA API Catalog</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!--
  SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  SPDX-License-Identifier: Apache-2.0

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<section class="tex2jax_ignore mathjax_ignore" id="using-the-nvidia-api-catalog">
<h1>Using the NVIDIA API Catalog<a class="headerlink" href="#using-the-nvidia-api-catalog" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#example-features" id="id1">Example Features</a></p></li>
<li><p><a class="reference internal" href="#prerequisites" id="id2">Prerequisites</a></p></li>
<li><p><a class="reference internal" href="#get-an-api-key-for-the-accessing-models-on-the-api-catalog" id="id3">Get an API Key for the Accessing Models on the API Catalog</a></p></li>
<li><p><a class="reference internal" href="#build-and-start-the-containers" id="id4">Build and Start the Containers</a></p></li>
<li><p><a class="reference internal" href="#using-an-alternative-inference-model" id="id5">Using an Alternative Inference Model</a></p></li>
<li><p><a class="reference internal" href="#using-the-llamaindex-data-framework" id="id6">Using the LlamaIndex Data Framework</a></p></li>
<li><p><a class="reference internal" href="#next-steps" id="id7">Next Steps</a></p></li>
</ul>
</div>
<section id="example-features">
<h2>Example Features<a class="headerlink" href="#example-features" title="Permalink to this headline"></a></h2>
<p>This example deploys a developer RAG pipeline for chat Q&amp;A and serves inferencing from an NVIDIA API Catalog endpoint
instead of a local inference server, a local model, or local GPUs.</p>
<p>Developers get free credits for 10K requests to any of the available models.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Embedding</p></th>
<th class="head"><p>Framework</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Multi-GPU</p></th>
<th class="head"><p>TRT-LLM</p></th>
<th class="head"><p>Model Location</p></th>
<th class="head"><p>NIM for LLMs</p></th>
<th class="head"><p>Vector Database</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ai-llama3-70b</p></td>
<td><p>snowflake/arctic-embed-l</p></td>
<td><p>LangChain</p></td>
<td><p>QA chatbot</p></td>
<td><p>NO</p></td>
<td><p>NO</p></td>
<td><p>API Catalog</p></td>
<td><p>NO</p></td>
<td><p>Milvus</p></td>
</tr>
<tr class="row-odd"><td><p>ai-llama3-8b</p></td>
<td><p>snowflake/arctic-embed-l</p></td>
<td><p>LlamaIndex</p></td>
<td><p>QA chatbot</p></td>
<td><p>NO</p></td>
<td><p>NO</p></td>
<td><p>API Catalog</p></td>
<td><p>NO</p></td>
<td><p>Milvus</p></td>
</tr>
</tbody>
</table>
<p>The following figure shows the sample topology:</p>
<ul class="simple">
<li><p>The sample chat bot web application communicates with the chain server.
The chain server sends inference requests to an NVIDIA API Catalog endpoint.</p></li>
<li><p>Optionally, you can deploy NVIDIA Riva. Riva can use automatic speech recognition to transcribe
your questions and use text-to-speech to speak the answers aloud.</p></li>
</ul>
<p><img alt="Using NVIDIA API Catalog endpoints for inference instead of local components." src="_images/ai-foundations-topology.png" /></p>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline"></a></h2>
<ul>
<li><p>Clone the Generative AI examples Git repository using Git LFS:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>apt<span class="w"> </span>-y<span class="w"> </span>install<span class="w"> </span>git-lfs
<span class="gp">$ </span>git<span class="w"> </span>clone<span class="w"> </span>git@github.com:NVIDIA/GenerativeAIExamples.git
<span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>GenerativeAIExamples/
<span class="gp">$ </span>git<span class="w"> </span>lfs<span class="w"> </span>pull
</pre></div>
</div>
</li>
<li><p>Install Docker Engine and Docker Compose.
Refer to the instructions for <a class="reference external" href="https://docs.docker.com/engine/install/ubuntu/">Ubuntu</a>.</p></li>
<li><p>Login to Nvidia’s docker registry. Please refer to <a class="reference external" href="https://docs.nvidia.com/ngc/gpu-cloud/ngc-overview/index.html">instructions</a> to create account and generate NGC API key. This is needed for pulling in the secure base container used by all the examples.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>login<span class="w"> </span>nvcr.io
<span class="go">Username: $oauthtoken</span>
<span class="go">Password: &lt;ngc-api-key&gt;</span>
</pre></div>
</div>
</li>
<li><p>Optional: Enable NVIDIA Riva automatic speech recognition (ASR) and text to speech (TTS).</p>
<ul>
<li><p>To launch a Riva server locally, refer to the <a class="reference external" href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html">Riva Quick Start Guide</a>.</p>
<ul class="simple">
<li><p>In the provided <code class="docutils literal notranslate"><span class="pre">config.sh</span></code> script, set <code class="docutils literal notranslate"><span class="pre">service_enabled_asr=true</span></code> and <code class="docutils literal notranslate"><span class="pre">service_enabled_tts=true</span></code>, and select the desired ASR and TTS languages by adding the appropriate language codes to <code class="docutils literal notranslate"><span class="pre">asr_language_code</span></code> and <code class="docutils literal notranslate"><span class="pre">tts_language_code</span></code>.</p></li>
<li><p>After the server is running, assign its IP address (or hostname) and port (50051 by default) to <code class="docutils literal notranslate"><span class="pre">RIVA_API_URI</span></code> in <code class="docutils literal notranslate"><span class="pre">deploy/compose/compose.env</span></code>.</p></li>
</ul>
</li>
<li><p>Alternatively, you can use a hosted Riva API endpoint. You might need to obtain an API key and/or Function ID for access.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">deploy/compose/compose.env</span></code>, make the following assignments as necessary:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">RIVA_API_URI</span><span class="o">=</span><span class="s2">&quot;&lt;riva-api-address/hostname&gt;:&lt;port&gt;&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RIVA_API_KEY</span><span class="o">=</span><span class="s2">&quot;&lt;riva-api-key&gt;&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RIVA_FUNCTION_ID</span><span class="o">=</span><span class="s2">&quot;&lt;riva-function-id&gt;&quot;</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
<section id="get-an-api-key-for-the-accessing-models-on-the-api-catalog">
<h2>Get an API Key for the Accessing Models on the API Catalog<a class="headerlink" href="#get-an-api-key-for-the-accessing-models-on-the-api-catalog" title="Permalink to this headline"></a></h2>
<p>Perform the following steps if you do not already have an API key.
You can use different model API endpoints with the same API key.</p>
<ol class="arabic">
<li><p>Navigate to <a class="reference external" href="https://build.nvidia.com/explore/discover">https://build.nvidia.com/explore/discover</a>.</p></li>
<li><p>Find the <strong>Llama 3 70B Instruct</strong> card and click the card.</p>
<p><img alt="Llama 3 70B Instruct model card" src="_images/llama3-70b-instruct-model-card.png" /></p>
</li>
<li><p>Click <strong>Get API Key</strong>.</p>
<p><img alt="API section of the model page." src="_images/llama3-70b-instruct-get-api-key.png" /></p>
</li>
<li><p>Click <strong>Generate Key</strong>.</p>
<p><img alt="Generate key window." src="_images/api-catalog-generate-api-key.png" /></p>
</li>
<li><p>Click <strong>Copy Key</strong> and then save the API key.
The key begins with the letters nvapi-.</p>
<p><img alt="Key Generated window." src="_images/key-generated.png" /></p>
</li>
</ol>
</section>
<section id="build-and-start-the-containers">
<h2>Build and Start the Containers<a class="headerlink" href="#build-and-start-the-containers" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>In the Generative AI examples repository, export this variable in terminal.</p>
<p>Add the API key for the model endpoint:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export NVIDIA_API_KEY=&quot;nvapi-&lt;...&gt;&quot;
</pre></div>
</div>
</li>
<li><p>From the root of the repository, build the containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-f<span class="w"> </span>deploy/compose/rag-app-api-catalog-text-chatbot.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>build
</pre></div>
</div>
</li>
<li><p>Start the containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-f<span class="w"> </span>deploy/compose/rag-app-api-catalog-text-chatbot.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>up<span class="w"> </span>-d
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go"> ✔ Network nvidia-rag         Created</span>
<span class="go"> ✔ Container chain-server     Started</span>
<span class="go"> ✔ Container rag-playground   Started</span>
</pre></div>
</div>
</li>
<li><p>Start the Milvus vector database:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-f<span class="w"> </span>deploy/compose/docker-compose-vectordb.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--profile<span class="w"> </span>llm-embedding<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>up<span class="w"> </span>-d<span class="w"> </span>milvus
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">✔ Container milvus-minio       Started</span>
<span class="go">✔ Container milvus-etcd        Started</span>
<span class="go">✔ Container milvus-standalone  Started</span>
</pre></div>
</div>
</li>
<li><p>Confirm the containers are running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>ps<span class="w"> </span>--format<span class="w"> </span><span class="s2">&quot;table {{.ID}}\t{{.Names}}\t{{.Status}}&quot;</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">CONTAINER ID   NAMES               STATUS</span>
<span class="go">39a8524829da   rag-playground      Up 2 minutes</span>
<span class="go">bfbd0193dbd2   chain-server        Up 2 minutes</span>
<span class="go">ec02ff3cc58b   milvus-standalone   Up 3 minutes</span>
<span class="go">6969cf5b4342   milvus-minio        Up 3 minutes (healthy)</span>
<span class="go">57a068d62fbb   milvus-etcd         Up 3 minutes (healthy)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="using-an-alternative-inference-model">
<h2>Using an Alternative Inference Model<a class="headerlink" href="#using-an-alternative-inference-model" title="Permalink to this headline"></a></h2>
<p>You can specify the model to use in the <code class="docutils literal notranslate"><span class="pre">APP_LLM_MODELNAME</span></code> environment variable when you start the Chain Server.
The following sample command uses the Mistral AI Mixtral 8x7B Instruct model.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nv">APP_LLM_MODELNAME</span><span class="o">=</span><span class="s1">&#39;mistralai/mixtral-8x7b-instruct-v0.1&#39;</span><span class="w"> </span>docker<span class="w"> </span>compose<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-f<span class="w"> </span>deploy/compose/rag-app-api-catalog-text-chatbot.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>up<span class="w"> </span>-d
</pre></div>
</div>
<p>You can determine the available model names using one of the following methods:</p>
<ul class="simple">
<li><p>Browse the models at <a class="reference external" href="https://build.ngc.nvidia.com/explore/discover">https://build.ngc.nvidia.com/explore/discover</a>.
View the sample Python code and get the model name from the <code class="docutils literal notranslate"><span class="pre">model</span></code> argument to the <code class="docutils literal notranslate"><span class="pre">client.chat.completions.create</span></code> method.</p></li>
<li><p>Install the <a class="reference external" href="https://pypi.org/project/langchain-nvidia-ai-endpoints/">langchain-nvidia-ai-endpoints</a> Python package from PyPi.
Use the <code class="docutils literal notranslate"><span class="pre">get_available_models()</span></code> method to list the models.
Refer to the preceding web page for sample code to list the models.</p></li>
</ul>
</section>
<section id="using-the-llamaindex-data-framework">
<h2>Using the LlamaIndex Data Framework<a class="headerlink" href="#using-the-llamaindex-data-framework" title="Permalink to this headline"></a></h2>
<p>As an alternative to the LangChain based Chain Server, you can build and run a LlamaIndex based Chain Server.</p>
<p>This example also starts a JupyterLab server on port 8888.</p>
<ol class="arabic">
<li><p>After meeting the <a class="reference internal" href="#prerequisites"><span class="std std-doc">Prerequisites</span></a>, build the containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>build
</pre></div>
</div>
</li>
<li><p>Start the containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>up<span class="w"> </span>-d
</pre></div>
</div>
</li>
<li><p>Start the Milvus vector database:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-f<span class="w"> </span>deploy/compose/docker-compose-vectordb.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--profile<span class="w"> </span>llm-embedding<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>up<span class="w"> </span>-d<span class="w"> </span>milvus
</pre></div>
</div>
</li>
</ol>
</section>
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Access the web interface for the chat server.
Refer to <a class="reference internal" href="using-sample-web-application.html"><span class="doc std std-doc">Using the Sample Chat Web Application</span></a> for information about using the web interface.</p></li>
<li><p><a class="reference internal" href="vector-database.html"><span class="doc std std-doc">Configuring an Alternative Vector Database</span></a></p></li>
<li><p>Stop the containers by running <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">compose</span> <span class="pre">-f</span> <span class="pre">deploy/compose/rag-app-api-catalog-text-chatbot.yaml</span> <span class="pre">down</span></code> and
<code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">compose</span> <span class="pre">-f</span> <span class="pre">deploy/compose/docker-compose-vectordb.yaml</span> <span class="pre">--profile</span> <span class="pre">llm-embedding</span> <span class="pre">down</span></code>.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="support-matrix.html" class="btn btn-neutral float-left" title="Support Matrix" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="query-decomposition.html" class="btn btn-neutral float-right" title="Query Decomposition" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA Corporation.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>